{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafb515b",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc22e2-63a0-4a2a-bcaf-54786597b821",
   "metadata": {},
   "source": [
    "The steps in this notebook include: \n",
    "- **Use Langchain Chroma Vectorstore and Lanchain Retrievers** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813cce6e-ac18-46a8-a497-0e91211e68f7",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Installation](#installation)\n",
    "2. [Similarity Search](#similarity)\n",
    "3. [Maximum marginal relevance (MRR)](#MRR)  \n",
    "4. [SelfQuery](#selfquery)\n",
    "5. [Compression](#compression)\n",
    "6. [Combining various techniques](#combining)\n",
    "7. [Other types of retrieval](#others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8698a-f38e-4526-ab55-7898efa001dd",
   "metadata": {},
   "source": [
    "**Source:** https://learn.deeplearning.ai/langchain-chat-with-your-data/lesson/5/retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bc0bd0-35fd-4533-9c90-25fde4070159",
   "metadata": {},
   "source": [
    "![overview.png](./images/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903fedf-75ac-415e-b7f5-a60092502b53",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b22d8-9f39-4df9-be1e-4599d8202f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Installation** <a name=\"installation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4ed4f8-6cf0-4e94-8a50-c173ad13fb16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.0.336)\n",
      "Requirement already satisfied: openai in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.0.0)\n",
      "Collecting lark\n",
      "  Downloading lark-1.1.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.8.6)\n",
      "Requirement already satisfied: anyio<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.6.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (0.0.64)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (1.22.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.5.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (0.25.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (3.1.1)\n",
      "Downloading lark-1.1.8-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.6/111.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain openai python-dotenv lark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e238f-ccc0-467b-a2cd-ed3231d630ca",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f9ad2a1-2c8e-471e-8174-338cf2e3e92d",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Load from a .env file \n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJhcHAiLCJzdWIiOiIxNDYyNzU5IiwiYXVkIjoiV0VCIiwiaWF0IjoxNjk5NDUxNzMzLCJleHAiOjE3MDAwNTY1MzN9.7mqcOZ3w4gd7m9QGWcdOx7U1ayk1l22LNZ8LfPOLqjE\"\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113914c6-6349-4c7f-a11d-3c3506f54840",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ac494-7173-4d4c-bc35-08a4c2758fab",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f11bda-1f98-48a6-a85d-ea72f8a08647",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Similarity Search** <a name=\"similarity\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dd1d92-de2a-4059-8274-1868a1a1d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf ./db/chroma  # remove old database files if any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8865712d-ef67-4a24-baae-ed9f899e831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "persist_directory = 'db/chroma/'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ddf5d-3f66-466f-aa1d-751fbad64cdb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> üí°<b>ChromaDB:</b>  \n",
    "    \n",
    "<b>ChromaDB</b> is an open-source vector store used for storing and retrieving vector embeddings. Its main use is to save embeddings along with metadata to be used later by large language models. Additionally, it can also be used for semantic search engines over text data. <a href=\"https://docs.trychroma.com/\">More</a>.  \n",
    "    \n",
    "(we should have the <code>chromadb</code> python package installed).\n",
    "\n",
    "<pre>\n",
    "# save to disk\n",
    "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# load from disk\n",
    "db2 = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n",
    "docs = db2.similarity_search(query)\n",
    "</pre>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d36d9-eed7-4e88-9a26-a018e8b0a80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"\"\"The Amanita phalloides has a large and imposing epigeous (aboveground) fruiting body (basidiocarp).\"\"\",\n",
    "    \"\"\"A mushroom with a large fruiting body is the Amanita phalloides. Some varieties are all-white.\"\"\",\n",
    "    \"\"\"A. phalloides, a.k.a Death Cap, is one of the most poisonous of all known mushrooms.\"\"\",\n",
    "]\n",
    "\n",
    "smalldb = Chroma.from_texts(texts, embedding=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9664f-4e48-42f3-ab8c-cb7cd726203c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> üí°<b>ChromaDB:</b>   \n",
    "\n",
    "Create a Chroma vectorstore from a list of <code>Documents</code>:\n",
    "<code>from_documents(documents[, embedding, ids, ...])</code>\n",
    "\n",
    "Create a Chroma vectorstore from a raw documents:\n",
    "<code>from_texts(texts[, embedding, metadatas, ...])</code>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598529c9-ee86-4380-83db-28d0625945b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Tell me about all-white mushrooms with large fruiting bodies\"\n",
    "\n",
    "print(smalldb.similarity_search(question, k=2),\"\\n\")\n",
    "print(smalldb.max_marginal_relevance_search(question,k=2, fetch_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f8c1c-e4e7-48eb-a09e-446a584d30f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <b>Similarity Search:</b>  \n",
    "    \n",
    "    Run similarity search with Chroma.\n",
    "\n",
    "        Args:\n",
    "            - query (str): Query text to search for.\n",
    "            - k (int): Number of results to return. Defaults to 4.\n",
    "            - filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
    "        \n",
    "        Returns:\n",
    "            - List[Document]: List of documents most similar (cosine distance) to the query text.\n",
    "    \n",
    "<br/>  \n",
    "<b>Similarity search by vector:</b> It is also possible to do a search for documents similar to a given embedding vector using <code>similarity_search_by_vector</code> which accepts an embedding vector as a parameter instead of a string. <br/> \n",
    "\n",
    "  \n",
    "<br/> \n",
    "<b>Maximum marginal relevance search (MMR):</b>  \n",
    "\n",
    "    Return docs selected using the maximal marginal relevance. Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.\n",
    "\n",
    "        Parameters\n",
    "            - query: Text to look up documents similar to.\n",
    "            - k: Number of Documents to return. Defaults to 4.\n",
    "            - fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
    "            - lambda_mult: Number between 0 and 1 that determines the degree\n",
    "                        of diversity among the results with 0 corresponding\n",
    "                        to maximum diversity and 1 to minimum diversity.\n",
    "                        Defaults to 0.5.\n",
    "            - filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            - List of Documents selected by maximal marginal relevance.\n",
    "\n",
    "<br/>  \n",
    "<b>Maximum marginal relevance search (MMR) by vector:</b> It is also possible to do a MMR using vector with <code>max_marginal_relevance_search_by_vector</code> which accepts an embedding vector as a parameter instead of a string. <br/> \n",
    "<br/>\n",
    "<i><b>Note:</i></b> MMR algorithm uses the <code>maximal_marginal_relevance()</code> funtion to calculate the maximal marginal relevance (similarity with <i>cosine simalirity</i>). From the <i>Utility functions</i> for working with vectors and vectorstores (<a href=\"https://api.python.langchain.com/en/latest/_modules/langchain/vectorstores/utils.html\">langchain.vectorstores.utils</a>)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb23de3-4338-4fdc-bc51-d5ddad96bba8",
   "metadata": {
    "height": 30
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675dcbff-6823-42f5-8c33-037139f4fcc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Maximum marginal relevance (MRR)** <a name=\"MRR\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf23c8d-c80a-4d17-a810-513c8bd9720a",
   "metadata": {},
   "source": [
    "In `Vectorstores_&_Embeddings_03.ipynb` we introduced one problem: how to enforce diversity in the search results.\n",
    " \n",
    "`Maximum marginal relevance` strives to achieve both relevance to the query *and diversity* among the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4aa47-4360-4ccd-b960-31fe6bf78716",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "docs_ss = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7df74c-f726-4ef1-bb3d-dad7816961b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220cc7ef-b1ec-4391-b8c5-ed7af3c1081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_ss[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021f6d1a-c6ae-4194-b1bc-34dcd91dd5d8",
   "metadata": {},
   "source": [
    "From the latest lab, we have **209 chunks** from 4 PDFs. Where each `page_content`'s length is <1500.\n",
    "````\n",
    "# Duplicate documents on purpose - messy data\n",
    "    \"data/MachineLearning-Lecture01.pdf\",\n",
    "    \"data/MachineLearning-Lecture01.pdf\",\n",
    "    \"data/MachineLearning-Lecture02.pdf\",\n",
    "    \"data/MachineLearning-Lecture03.pdf\"\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c7779-6f93-4cef-9965-bd5979264a1a",
   "metadata": {},
   "source": [
    "Note the difference in results with `MMR`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49584b24-aec4-4178-b7ff-dad9d000608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500d434-c377-4b5d-93e9-024d1bc39b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e067a5-fa98-4a92-a631-9b7b37d17d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr[1].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529da04-f0e2-4e8c-aef0-910c96dc25e9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18439d-1b16-4ea4-a8ad-16a492bf055f",
   "metadata": {},
   "source": [
    "# **SelfQuery** <a name=\"selfquery\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263930f-f93d-42e4-a240-d9c999ed7114",
   "metadata": {},
   "source": [
    "**Working with metadata**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edebb9-dda0-4d2d-8d6a-fbd23e74c04a",
   "metadata": {},
   "source": [
    "We showed that a question about the one lecture can include results from other lectures as well. To address this, many vectorstores support _operations_ on **metadata**.\n",
    "\n",
    "- **metadata** provides context for each embedded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d81dc8-4ad4-4a5b-ac09-fbfa9011b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"docs/cs229_lectures/MachineLearning-Lecture03.pdf\"}\n",
    ")\n",
    "\n",
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6db56-c3f1-403a-bd09-ad20979974c4",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2f225-4401-4cbb-a0b9-35dc8017a5f6",
   "metadata": {},
   "source": [
    "**Working with metadata using self-query retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f2b99-f629-43c0-a595-64d272413b74",
   "metadata": {},
   "source": [
    "But we can infer the metadata from the query itself. To address this, we can use `SelfQueryRetriever`, which uses an LLM to extract:\n",
    " \n",
    "1. The `query` string to use for vector search\n",
    "2. A metadata filter to pass in as well\n",
    "\n",
    "Most vector databases support metadata filters, so this doesn't require any new databases or indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca6539-c557-4765-a9e7-7b8ab709bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The lecture the chunk is from, should be one of `docs/cs229_lectures/MachineLearning-Lecture01.pdf`, `docs/cs229_lectures/MachineLearning-Lecture02.pdf`, or `docs/cs229_lectures/MachineLearning-Lecture03.pdf`\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the lecture\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b795e77-2ce7-4adf-afdc-6da1d3608387",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content_description = \"Lecture notes\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90b7898-893e-4542-97ac-82d4889c4394",
   "metadata": {},
   "source": [
    "We will receive a warning about `predict_and_parse` being deprecated the first time we executing the next line. This can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77d15f-2b24-4155-9eac-7de7092197d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about regression in the third lecture?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36310600-35c6-448c-a4e5-fee4aa4bbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca8051-0884-4651-9dec-f01eef34a5a9",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dd63e7-01b8-492f-911b-7c4d28951f34",
   "metadata": {},
   "source": [
    "***Note:*** We saw in the first Lab that `PyPDFLoader` creates `Documents`that contain text (page_content) and metadata.\n",
    "````\n",
    "> page\n",
    "    Document(page_content='MachineLearning-Lecture01  \\nInstru...')\n",
    "    \n",
    "> page.metadata\n",
    "    {'source': 'data/MachineLearning-Lecture01.pdf', 'page': 0}\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3bc71-6e81-42b1-8d75-ead5d3cb5c4b",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b78d63-424a-4c74-b718-38b8d2bcf155",
   "metadata": {},
   "source": [
    "# **Compression** <a name=\"compression\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5131b3e-9549-4211-8963-8727f7e98969",
   "metadata": {},
   "source": [
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "\n",
    "Information most relevant to a query may be buried in a document with a lot of **irrelevant text**.\n",
    "\n",
    "Passing that full document through your application can lead to **more expensive LLM calls and poorer responses**.  \n",
    "--> Contextual compression is meant to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6452b7-0e57-48ff-932c-29d982d5c87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d1466-7c5d-4278-8547-92ab4d9a6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252596b-f68b-4099-adf8-dc3ef9eeecc7",
   "metadata": {},
   "source": [
    "LLM wrappers are simply an intermediate that allows one to connect to a Large Language Model.\n",
    "\n",
    "**LLMChainExtractor**: Document compressor that uses an LLM chain to extract the relevant parts of documents.\n",
    "\n",
    "**ContextualCompressionRetriever**: Retriever that wraps a base retriever and compresses the results.\n",
    "\n",
    ">**Parameters:** \n",
    ">- `base_compressor` _langchain.retrievers.document_compressors.base.BaseDocumentCompressor_ ‚Äì Compressor for compressing retrieved documents.\n",
    ">- `base_retriever` _langchain.schema.retriever.BaseRetriever_ ‚Äì Base Retriever to use for getting relevant documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db553f-3158-43c1-8a64-c76773b178e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef28e37-51ec-4672-8feb-5cdd493e456f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc56422-96ec-445c-8bc2-41aaa399b9d2",
   "metadata": {},
   "source": [
    "# **Combining various techniques** <a name=\"combining\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1f2ff-fcea-4a3c-8f7b-889253405679",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefd8fd-1f72-4645-ad70-0edde70554e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd382a48-72c5-4cb4-aac1-85f4ec5c5ae0",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f680c5-c5e2-4318-83c8-0fe21e502433",
   "metadata": {},
   "source": [
    "# **Other types of retrieval** <a name=\"others\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659ce6d-76b4-441f-9e7b-ca67fb9386d3",
   "metadata": {},
   "source": [
    "It's worth noting that vectordb as not the only kind of tool to retrieve documents.\n",
    "\n",
    "The LangChain retriever abstraction includes other ways to retrieve documents, such as TF-IDF or SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579fd45-f363-4fd5-8fc6-0d2bdda05125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import SVMRetriever\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"data/MachineLearning-Lecture01.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "# Retrieve\n",
    "svm_retriever = SVMRetriever.from_texts(splits,embedding)\n",
    "tfidf_retriever = TFIDFRetriever.from_texts(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042c1f1-92af-456e-8235-7a2d28af61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are major topics for this class?\"\n",
    "\n",
    "docs_svm=svm_retriever.get_relevant_documents(question)\n",
    "print(docs_svm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1127f7-7689-45b0-be9a-aa8a3289c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what did they say about matlab?\"\n",
    "\n",
    "docs_tfidf=tfidf_retriever.get_relevant_documents(question)\n",
    "print(docs_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ddccf-507c-4ff7-af40-771b208d684f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c09048-14b9-4786-bd4c-45fdd59fa279",
   "metadata": {},
   "source": [
    "By looking at the entire dataset and separating positive and negative examples through an optimally positioned hyperplane, SVM is capable of providing high-quality results for complex query types.\n",
    "\n",
    "Similarities between **KNN** and **SVM**:\n",
    "- Both are supervised machine learning algorithms.\n",
    "- They can be used for various NLP tasks, including classification or information retrieval.\n",
    "- They work well with high-dimensional data, such as sentence embeddings.\n",
    "\n",
    "Differences between **KNN** and **SVM**:\n",
    "- KNN is a lazy learning technique that takes into account only the K nearest neighbors, whereas SVM is a model-based technique that considers the overall structure of the entire dataset.\n",
    "- KNN emphasizes local similarity, while SVM focuses on global relationships and margins between classes.\n",
    "- KNN generally has a faster training phase as it doesn‚Äôt involve model building, but SVM can be computationally expensive and slower during training due to hyperplane fitting.  \n",
    "\n",
    "[More](https://blog.gopenai.com/knowing-your-neighbors-or-harnessing-support-selecting-knn-or-svm-for-prompt-engineering-d43807580753)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
